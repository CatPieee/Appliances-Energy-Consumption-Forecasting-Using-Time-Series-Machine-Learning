{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ff7ec5",
   "metadata": {},
   "source": [
    "# Time Series Machine Learning Experimentation\n",
    "\n",
    "This notebook implements various time series forecasting models for appliances energy consumption prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import_packages",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987877fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Time series models\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Deep learning for time series\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Plotting\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800f0db1",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076ba70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df = pd.read_csv('../data/energydata_complete_cleaned.csv', parse_dates=['date'], index_col='date')\n",
    "df.sort_index(inplace=True)\n",
    "print('Data shape: ', df.shape)\n",
    "print('Date range:', df.index.min(), 'to', df.index.max())\n",
    "\n",
    "# Define features and target variable\n",
    "features = ['lights', 'T1', 'RH_1', 'T2', 'RH_2', 'T3', 'RH_3', 'T4', 'RH_4', \n",
    "            'T5', 'RH_5', 'T6', 'RH_6', 'T7', 'RH_7', 'T8', 'RH_8', 'T9', \n",
    "            'RH_9', 'T_out', 'Press_mm_hg', 'RH_out', 'Windspeed', \n",
    "            'Visibility', 'Tdewpoint', 'hour_of_day', 'day_of_week', \n",
    "            'is_weekend', 'hour_sin', 'hour_cos', 'day_of_week_sin', \n",
    "            'day_of_week_cos', 'Appliances_lag1', 'Appliances_rolling_mean_6']\n",
    "target = 'Appliances'\n",
    "print('Feature number: ', len(features))\n",
    "\n",
    "# For time series models, we'll use chronological split\n",
    "train_size = int(len(df) * 0.7)\n",
    "train_df = df.iloc[:train_size].copy()\n",
    "test_df = df.iloc[train_size:].copy()\n",
    "\n",
    "print('Training data shape: ', train_df.shape)\n",
    "print('Testing data shape: ', test_df.shape)\n",
    "print('Training period:', train_df.index.min(), 'to', train_df.index.max())\n",
    "print('Testing period:', test_df.index.min(), 'to', test_df.index.max())\n",
    "\n",
    "# Prepare Prophet format data\n",
    "prophet_train = train_df.reset_index().rename(columns={'date': 'ds', target: 'y'})[['ds', 'y']]\n",
    "prophet_test = test_df.reset_index().rename(columns={'date': 'ds', target: 'y'})[['ds', 'y']]\n",
    "\n",
    "print('\\nProphet training data shape:', prophet_train.shape)\n",
    "print('Prophet testing data shape:', prophet_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b0f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results dictionary\n",
    "model_results = {}\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Calculate evaluation metrics for a model\"\"\"\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    results = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    model_results[model_name] = results\n",
    "    \n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"MSE: {mse:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RÂ²: {r2:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95aa3029",
   "metadata": {},
   "source": [
    "## 1. Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99523c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Prophet model...\")\n",
    "\n",
    "# Initialize Prophet model\n",
    "prophet_model = Prophet(\n",
    "    daily_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    yearly_seasonality=False,  # no yearly seasonality due to limited data period\n",
    "    changepoint_prior_scale=0.05,\n",
    "    seasonality_prior_scale=10.0,\n",
    "    holidays_prior_scale=10.0,\n",
    "    seasonality_mode='multiplicative'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "prophet_model.fit(prophet_train)\n",
    "\n",
    "# Make predictions\n",
    "prophet_forecast = prophet_model.predict(prophet_test[['ds']])\n",
    "prophet_predictions = prophet_forecast['yhat'].values\n",
    "\n",
    "# Evaluate Prophet model\n",
    "prophet_results = evaluate_model(prophet_test['y'].values, prophet_predictions, 'Prophet')\n",
    "\n",
    "print(\"Prophet model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prophet_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Prophet results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Forecast vs Actual\n",
    "axes[0].plot(prophet_test['ds'], prophet_test['y'], label='Actual', color='blue', alpha=0.7)\n",
    "axes[0].plot(prophet_test['ds'], prophet_predictions, label='Prophet Forecast', color='red', alpha=0.7)\n",
    "axes[0].set_title('Prophet Model: Actual vs Predicted Appliances Energy Consumption')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Energy Consumption (Wh)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "residuals = prophet_test['y'].values - prophet_predictions\n",
    "axes[1].scatter(prophet_predictions, residuals, alpha=0.6)\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1].set_title('Prophet Model: Residuals Plot')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/prediction_plots/prophet_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Prophet components\n",
    "fig = prophet_model.plot_components(prophet_forecast)\n",
    "plt.savefig('../results/prediction_plots/prophet_components.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arima_section",
   "metadata": {},
   "source": [
    "## 2. ARIMA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arima_stationarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stationarity of the time series\n",
    "def check_stationarity(timeseries, title):\n",
    "    \"\"\"Check if time series is stationary using Augmented Dickey-Fuller test\"\"\"\n",
    "    print(f'Results of Augmented Dickey-Fuller Test for {title}:')\n",
    "    dftest = adfuller(timeseries, autolag='AIC')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "    if dftest[1] <= 0.05:\n",
    "        print(\"Series is stationary\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"Series is not stationary\")\n",
    "        return False\n",
    "\n",
    "# Check original series\n",
    "appliances_series = train_df[target]\n",
    "is_stationary = check_stationarity(appliances_series, \"Original Appliances Series\")\n",
    "\n",
    "# If not stationary, apply differencing\n",
    "if not is_stationary:\n",
    "    appliances_diff = appliances_series.diff().dropna()\n",
    "    print(\"\\nAfter first differencing:\")\n",
    "    is_stationary_diff = check_stationarity(appliances_diff, \"First Differenced Series\")\n",
    "    \n",
    "    if is_stationary_diff:\n",
    "        d_param = 1\n",
    "        working_series = appliances_diff\n",
    "    else:\n",
    "        appliances_diff2 = appliances_diff.diff().dropna()\n",
    "        print(\"\\nAfter second differencing:\")\n",
    "        check_stationarity(appliances_diff2, \"Second Differenced Series\")\n",
    "        d_param = 2\n",
    "        working_series = appliances_diff2\n",
    "else:\n",
    "    d_param = 0\n",
    "    working_series = appliances_series\n",
    "\n",
    "print(f\"\\nUsing d parameter: {d_param}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arima_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training ARIMA model...\")\n",
    "\n",
    "# Fit ARIMA model with auto-selected parameters\n",
    "# Using a simple ARIMA(2,1,2) as a starting point\n",
    "try:\n",
    "    arima_model = ARIMA(train_df[target], order=(2, d_param, 2))\n",
    "    arima_fitted = arima_model.fit()\n",
    "    \n",
    "    print(\"ARIMA model summary:\")\n",
    "    print(arima_fitted.summary())\n",
    "    \n",
    "    # Make predictions\n",
    "    arima_forecast = arima_fitted.forecast(steps=len(test_df))\n",
    "    arima_predictions = arima_forecast.values\n",
    "    \n",
    "    # Ensure predictions are non-negative (energy consumption can't be negative)\n",
    "    arima_predictions = np.maximum(arima_predictions, 0)\n",
    "    \n",
    "    # Evaluate ARIMA model\n",
    "    arima_results = evaluate_model(test_df[target].values, arima_predictions, 'ARIMA')\n",
    "    \n",
    "    print(\"ARIMA model training completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error training ARIMA model: {e}\")\n",
    "    print(\"Trying simpler ARIMA(1,1,1) model...\")\n",
    "    \n",
    "    try:\n",
    "        arima_model = ARIMA(train_df[target], order=(1, 1, 1))\n",
    "        arima_fitted = arima_model.fit()\n",
    "        \n",
    "        arima_forecast = arima_fitted.forecast(steps=len(test_df))\n",
    "        arima_predictions = arima_forecast.values\n",
    "        arima_predictions = np.maximum(arima_predictions, 0)\n",
    "        \n",
    "        arima_results = evaluate_model(test_df[target].values, arima_predictions, 'ARIMA')\n",
    "        print(\"ARIMA(1,1,1) model training completed!\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"Error with ARIMA(1,1,1): {e2}\")\n",
    "        arima_predictions = np.full(len(test_df), train_df[target].mean())\n",
    "        arima_results = evaluate_model(test_df[target].values, arima_predictions, 'ARIMA (Mean Baseline)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arima_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ARIMA results\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 12))\n",
    "\n",
    "# Plot 1: Forecast vs Actual\n",
    "axes[0].plot(test_df.index, test_df[target], label='Actual', color='blue', alpha=0.7)\n",
    "axes[0].plot(test_df.index, arima_predictions, label='ARIMA Forecast', color='green', alpha=0.7)\n",
    "axes[0].set_title('ARIMA Model: Actual vs Predicted Appliances Energy Consumption')\n",
    "axes[0].set_xlabel('Date')\n",
    "axes[0].set_ylabel('Energy Consumption (Wh)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Residuals\n",
    "arima_residuals = test_df[target].values - arima_predictions\n",
    "axes[1].scatter(arima_predictions, arima_residuals, alpha=0.6, color='green')\n",
    "axes[1].axhline(y=0, color='red', linestyle='--')\n",
    "axes[1].set_title('ARIMA Model: Residuals Plot')\n",
    "axes[1].set_xlabel('Predicted Values')\n",
    "axes[1].set_ylabel('Residuals')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/prediction_plots/arima_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lstm_section",
   "metadata": {},
   "source": [
    "## 3. LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm_data_prep",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for LSTM\n",
    "def create_lstm_dataset(data, target_col, features, lookback=24):\n",
    "    \"\"\"Create dataset for LSTM with lookback window\"\"\"\n",
    "    # Scale the data\n",
    "    scaler_X = MinMaxScaler()\n",
    "    scaler_y = MinMaxScaler()\n",
    "    \n",
    "    X_scaled = scaler_X.fit_transform(data[features])\n",
    "    y_scaled = scaler_y.fit_transform(data[target_col].values.reshape(-1, 1))\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(lookback, len(X_scaled)):\n",
    "        X.append(X_scaled[i-lookback:i])\n",
    "        y.append(y_scaled[i])\n",
    "    \n",
    "    return np.array(X), np.array(y), scaler_X, scaler_y\n",
    "\n",
    "print(\"Preparing LSTM data...\")\n",
    "\n",
    "# Use a subset of most important features for LSTM\n",
    "lstm_features = ['lights', 'T_out', 'RH_out', 'hour_of_day', 'day_of_week', \n",
    "                 'is_weekend', 'Appliances_lag1', 'Appliances_rolling_mean_6']\n",
    "\n",
    "lookback_window = 24  # Use 24 time steps (4 hours) to predict next step\n",
    "\n",
    "# Prepare training data\n",
    "X_train_lstm, y_train_lstm, scaler_X, scaler_y = create_lstm_dataset(\n",
    "    train_df, target, lstm_features, lookback_window\n",
    ")\n",
    "\n",
    "print(f\"LSTM training data shape: X={X_train_lstm.shape}, y={y_train_lstm.shape}\")\n",
    "print(f\"Features used: {len(lstm_features)}\")\n",
    "print(f\"Lookback window: {lookback_window} time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm_model",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Building and training LSTM model...\")\n",
    "\n",
    "# Build LSTM model\n",
    "lstm_model = Sequential([\n",
    "    LSTM(50, return_sequences=True, input_shape=(lookback_window, len(lstm_features))),\n",
    "    Dropout(0.2),\n",
    "    LSTM(50, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(25),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "print(\"LSTM Model Architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_split=0.2,\n",
    "    verbose=1,\n",
    "    shuffle=False  # Important for time series\n",
    ")\n",
    "\n",
    "print(\"LSTM model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm_prediction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make LSTM predictions\n",
    "print(\"Making LSTM predictions...\")\n",
    "\n",
    "# Prepare test data for LSTM\n",
    "# We need to use the last lookback_window points from training data to start predictions\n",
    "full_data = pd.concat([train_df, test_df])\n",
    "test_start_idx = len(train_df)\n",
    "\n",
    "lstm_predictions = []\n",
    "current_batch = scaler_X.transform(full_data[lstm_features].iloc[test_start_idx-lookback_window:test_start_idx].values)\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    # Predict next value\n",
    "    current_pred = lstm_model.predict(current_batch.reshape(1, lookback_window, len(lstm_features)), verbose=0)\n",
    "    lstm_predictions.append(current_pred[0, 0])\n",
    "    \n",
    "    # Update batch for next prediction\n",
    "    if i < len(test_df) - 1:\n",
    "        # Use actual next values to update the batch (teacher forcing for evaluation)\n",
    "        next_features = scaler_X.transform(full_data[lstm_features].iloc[test_start_idx+i+1:test_start_idx+i+2].values)\n",
    "        current_batch = np.append(current_batch[1:], next_features, axis=0)\n",
    "\n",
    "# Inverse transform predictions\n",
    "lstm_predictions = np.array(lstm_predictions).reshape(-1, 1)\n",
    "lstm_predictions = scaler_y.inverse_transform(lstm_predictions).flatten()\n",
    "\n",
    "# Ensure predictions are non-negative\n",
    "lstm_predictions = np.maximum(lstm_predictions, 0)\n",
    "\n",
    "# Evaluate LSTM model\n",
    "lstm_results = evaluate_model(test_df[target].values, lstm_predictions, 'LSTM')\n",
    "\n",
    "print(\"LSTM predictions completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lstm_viz",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LSTM results\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 18))\n",
    "\n",
    "# Plot 1: Training history\n",
    "axes[0].plot(history.history['loss'], label='Training Loss')\n",
    "axes[0].plot(history.history['val_loss'], label='Validation Loss')\n",
    "axes[0].set_title('LSTM Model Training History')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Forecast vs Actual\n",
    "axes[1].plot(test_df.index, test_df[target], label='Actual', color='blue', alpha=0.7)\n",
    "axes[1].plot(test_df.index, lstm_predictions, label='LSTM Forecast', color='orange', alpha=0.7)\n",
    "axes[1].set_title('LSTM Model: Actual vs Predicted Appliances Energy Consumption')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Energy Consumption (Wh)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals\n",
    "lstm_residuals = test_df[target].values - lstm_predictions\n",
    "axes[2].scatter(lstm_predictions, lstm_residuals, alpha=0.6, color='orange')\n",
    "axes[2].axhline(y=0, color='red', linestyle='--')\n",
    "axes[2].set_title('LSTM Model: Residuals Plot')\n",
    "axes[2].set_xlabel('Predicted Values')\n",
    "axes[2].set_ylabel('Residuals')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/prediction_plots/lstm_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_section",
   "metadata": {},
   "source": [
    "## 4. Model Comparison and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "model_comparison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison\n",
    "print(\"=\" * 60)\n",
    "print(\"TIME SERIES MODELS PERFORMANCE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(model_results).T\n",
    "results_df = results_df.round(4)\n",
    "\n",
    "print(\"\\nDetailed Results:\")\n",
    "print(results_df)\n",
    "\n",
    "# Find best model for each metric\n",
    "print(\"\\nBest Models by Metric:\")\n",
    "print(f\"Lowest RMSE: {results_df['RMSE'].idxmin()} ({results_df['RMSE'].min():.2f})\")\n",
    "print(f\"Lowest MAE: {results_df['MAE'].idxmin()} ({results_df['MAE'].min():.2f})\")\n",
    "print(f\"Highest RÂ²: {results_df['R2'].idxmax()} ({results_df['R2'].max():.4f})\")\n",
    "\n",
    "# Overall ranking (based on RÂ² score)\n",
    "ranking = results_df.sort_values('R2', ascending=False)\n",
    "print(\"\\nOverall Ranking (by RÂ² score):\")\n",
    "for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {model}: RÂ² = {row['R2']:.4f}, RMSE = {row['RMSE']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final_visualization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Plot 1: All predictions comparison\n",
    "axes[0, 0].plot(test_df.index, test_df[target], label='Actual', color='black', linewidth=2, alpha=0.8)\n",
    "axes[0, 0].plot(test_df.index, prophet_predictions, label='Prophet', color='red', alpha=0.7)\n",
    "axes[0, 0].plot(test_df.index, arima_predictions, label='ARIMA', color='green', alpha=0.7)\n",
    "axes[0, 0].plot(test_df.index, lstm_predictions, label='LSTM', color='orange', alpha=0.7)\n",
    "axes[0, 0].set_title('Time Series Models Comparison: Predictions vs Actual', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Date')\n",
    "axes[0, 0].set_ylabel('Energy Consumption (Wh)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Performance metrics comparison\n",
    "metrics = ['RMSE', 'MAE', 'R2']\n",
    "x_pos = np.arange(len(results_df.index))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    if metric == 'R2':\n",
    "        # For RÂ², higher is better, so we can plot directly\n",
    "        axes[0, 1].bar(x_pos + i*width, results_df[metric], width, \n",
    "                      label=metric, alpha=0.8)\n",
    "    else:\n",
    "        # For RMSE and MAE, lower is better, so we normalize for visualization\n",
    "        normalized_values = 1 / (1 + results_df[metric] / results_df[metric].max())\n",
    "        axes[0, 1].bar(x_pos + i*width, normalized_values, width, \n",
    "                      label=f'{metric} (normalized)', alpha=0.8)\n",
    "\n",
    "axes[0, 1].set_title('Model Performance Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Models')\n",
    "axes[0, 1].set_ylabel('Performance Score')\n",
    "axes[0, 1].set_xticks(x_pos + width)\n",
    "axes[0, 1].set_xticklabels(results_df.index, rotation=45)\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Residuals comparison\n",
    "residuals_data = {\n",
    "    'Prophet': test_df[target].values - prophet_predictions,\n",
    "    'ARIMA': test_df[target].values - arima_predictions,\n",
    "    'LSTM': test_df[target].values - lstm_predictions\n",
    "}\n",
    "\n",
    "axes[1, 0].boxplot(residuals_data.values(), labels=residuals_data.keys())\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', alpha=0.7)\n",
    "axes[1, 0].set_title('Residuals Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Scatter plot of predictions vs actual\n",
    "axes[1, 1].scatter(test_df[target], prophet_predictions, alpha=0.6, label='Prophet', color='red')\n",
    "axes[1, 1].scatter(test_df[target], arima_predictions, alpha=0.6, label='ARIMA', color='green')\n",
    "axes[1, 1].scatter(test_df[target], lstm_predictions, alpha=0.6, label='LSTM', color='orange')\n",
    "\n",
    "# Perfect prediction line\n",
    "min_val = min(test_df[target].min(), min(prophet_predictions.min(), arima_predictions.min(), lstm_predictions.min()))\n",
    "max_val = max(test_df[target].max(), max(prophet_predictions.max(), arima_predictions.max(), lstm_predictions.max()))\n",
    "axes[1, 1].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.7, label='Perfect Prediction')\n",
    "\n",
    "axes[1, 1].set_title('Predicted vs Actual Values', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Actual Values')\n",
    "axes[1, 1].set_ylabel('Predicted Values')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/prediction_plots/timeseries_models_comprehensive_comparison.png', \n",
    "            dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('../results/timeseries_model_results.csv')\n",
    "print(\"\\nResults saved to '../results/timeseries_model_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusions",
   "metadata": {},
   "source": [
    "## 5. Conclusions and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conclusions_analysis",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TIME SERIES FORECASTING ANALYSIS - CONCLUSIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "best_model = results_df['R2'].idxmax()\n",
    "best_r2 = results_df['R2'].max()\n",
    "best_rmse = results_df.loc[best_model, 'RMSE']\n",
    "\n",
    "print(f\"\\nð BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"   - RÂ² Score: {best_r2:.4f}\")\n",
    "print(f\"   - RMSE: {best_rmse:.2f} Wh\")\n",
    "print(f\"   - MAE: {results_df.loc[best_model, 'MAE']:.2f} Wh\")\n",
    "\n",
    "print(\"\\nð KEY INSIGHTS:\")\n",
    "\n",
    "# Model-specific insights\n",
    "if 'Prophet' in model_results:\n",
    "    prophet_r2 = model_results['Prophet']['R2']\n",
    "    print(f\"\\nð® Prophet Model (RÂ² = {prophet_r2:.4f}):\")\n",
    "    print(\"   - Excellent at capturing seasonal patterns\")\n",
    "    print(\"   - Handles daily and weekly seasonality well\")\n",
    "    print(\"   - Robust to missing data and outliers\")\n",
    "    if prophet_r2 > 0.6:\n",
    "        print(\"   - Shows strong predictive performance\")\n",
    "    else:\n",
    "        print(\"   - May need additional regressors for better performance\")\n",
    "\n",
    "if 'ARIMA' in model_results:\n",
    "    arima_r2 = model_results['ARIMA']['R2']\n",
    "    print(f\"\\nð ARIMA Model (RÂ² = {arima_r2:.4f}):\")\n",
    "    print(\"   - Classical time series approach\")\n",
    "    print(\"   - Good for capturing linear trends and patterns\")\n",
    "    if arima_r2 < 0.3:\n",
    "        print(\"   - May struggle with complex non-linear patterns\")\n",
    "        print(\"   - Could benefit from more sophisticated parameter tuning\")\n",
    "    else:\n",
    "        print(\"   - Shows reasonable performance for linear patterns\")\n",
    "\n",
    "if 'LSTM' in model_results:\n",
    "    lstm_r2 = model_results['LSTM']['R2']\n",
    "    print(f\"\\nð§  LSTM Model (RÂ² = {lstm_r2:.4f}):\")\n",
    "    print(\"   - Deep learning approach for complex patterns\")\n",
    "    print(\"   - Can capture non-linear relationships\")\n",
    "    if lstm_r2 > 0.5:\n",
    "        print(\"   - Successfully learned complex temporal dependencies\")\n",
    "    else:\n",
    "        print(\"   - May need more training data or architecture tuning\")\n",
    "    print(\"   - Requires more computational resources\")\n",
    "\n",
    "print(\"\\nð¯ RECOMMENDATIONS:\")\n",
    "print(f\"\\n1. For production deployment, use {best_model} model\")\n",
    "print(\"2. Consider ensemble methods combining multiple models\")\n",
    "print(\"3. Monitor model performance and retrain periodically\")\n",
    "print(\"4. Collect more external features (weather, occupancy) for better accuracy\")\n",
    "\n",
    "if best_r2 < 0.7:\n",
    "    print(\"\\nâ ï¸  IMPROVEMENT OPPORTUNITIES:\")\n",
    "    print(\"   - Add more external regressors (weather, occupancy patterns)\")\n",
    "    print(\"   - Consider hybrid models combining multiple approaches\")\n",
    "    print(\"   - Implement more sophisticated feature engineering\")\n",
    "    print(\"   - Use ensemble methods for better robustness\")\n",
    "\n",
    "print(\"\\nð TECHNICAL NOTES:\")\n",
    "print(\"   - All models evaluated on chronological test split (30% of data)\")\n",
    "print(\"   - Predictions constrained to non-negative values\")\n",
    "print(\"   - LSTM used 24-step lookback window (4 hours)\")\n",
    "print(\"   - Prophet configured for daily/weekly seasonality\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ANALYSIS COMPLETE - All results saved to ../results/ directory\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
