import torch #æ‰‹ç”µç­’
from torch import nn    # neural network
from torch.utils.data import Dataset, DataLoader   # æ•°æ®åŠ è½½å™¨ â‰ˆ pandas æ•°æ®é›†åŠ è½½ï¼Œé¢„å¤„ç†ï¼Œåˆ’åˆ†
from matplotlib import pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

df_dataset = pd.read_csv('data/energydata_complete.csv')
print("æ•°æ®é›†çš„æ ·æœ¬å°ºå¯¸ï¼š", df_dataset.shape) # (19735, 29)
X = df_dataset.iloc[:, 2:-1]  # ç‰¹å¾
y = df_dataset.iloc[:, 1]  # ç›®æ ‡å˜é‡
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# æ•°æ®æ ‡å‡†åŒ–
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# æ•°æ®è½¬tensor
X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)
y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)
X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)
y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)

class MyDataset(Dataset):
    def __init__(self, X, y):
        self.X = X
        self.y = y

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):          # index ç´¢å¼•
        return self.X[idx], self.y[idx]

train_dataset = MyDataset(X_train_tensor, y_train_tensor)
test_dataset = MyDataset(X_test_tensor, y_test_tensor)
print("è®­ç»ƒé›†å°ºå¯¸: ", train_dataset.__len__()) # torch.Size([64, 26])
print("æµ‹è¯•é›†å°ºå¯¸: ", test_dataset.__len__())  # torch.Size([64])

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)

for x, y in train_loader:
    print("Shape of x [N, feature]: ", x.shape)
    print("y: ", y.shape)
    print("æŸ¥çœ‹ç¬¬ä¸€ä¸ªæ•°æ®ï¼š")
    print("x: ", x[0])
    print("y: ", y[0])
    break

device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else "cpu" # è®¡ç®—è®¾å¤‡ï¼Œç”¨cpu è¿˜æ˜¯gpu

# Define model
class MyNeuralNetwork(nn.Module): # çˆ¶ç±»ï¼Œç»§æ‰¿nn.Module æ‰èƒ½pytorchçš„åŠŸèƒ½
    def __init__(self):  #åˆå§‹åŒ–æ–¹æ³•
        super().__init__()  # superæ˜¯è°ƒç”¨çˆ¶ç±»çš„æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰
        # å‡†å¤‡ç¥ç»ç½‘ç»œçš„å„ä¸ªå±‚
        #self.flatten = nn.Flatten()  # åˆ›é€ ä¸€ä¸ªç±»çš„å±æ€§ï¼Œæ˜¯nnçš„Flatten å±•å¼€å±‚  æŠŠåé¢çš„èµ‹å€¼ç»™å·¦è¾¹ åé¢çš„æ˜¯ç”µç­’ğŸ”¦çš„æ–¹æ³•ï¼Œç›´æ¥è°ƒç”¨ï¼Œæ ‡å‡†åŒ–æ–¹æ³•ï¼ˆåªæœ‰å›¾åƒéœ€è¦ï¼‰
        self.linear_relu_stack = nn.Sequential( # nn.Sequential()ç”¨äºæŠŠä¸€ç»„ç¥ç»ç½‘ç»œå±‚æŒ‰é¡ºåºè¿æ¥
            nn.Linear(26, 10),         # çº¿æ€§ç¥ç»ç½‘ç»œ  output éšæœº
            nn.ReLU(),                                 # éçº¿å½¢æ¿€æ´»å‡½æ•°
            nn.Linear(10, 1), # çº¿æ€§ç¥ç»ç½‘ç»œ
            #nn.ReLU(),                                 #éçº¿å½¢æ¿€æ´»å‡½æ•°
            #nn.Linear()  # çº¿æ€§ç¥ç»ç½‘ç»œ
        )

    def forward(self, x): # çˆ¶ç±»æ–¹æ³•ï¼Œé‡å†™ å®ç°è‡ªå·±çš„èº«å½¢ç½‘ç»œï¼Œå‰å‘ä¼ æ’­ self = æ˜¯è¿™ä¸ªclassçš„æ–¹æ³•ï¼ˆå‡½æ•°ï¼‰è°ƒç”¨

        # x = self.flatten(x)  #å±•å¹³ï¼Œåªæœ‰å›¾åƒéœ€è¦
        #print("å±•å¼€åæ•°æ®çš„å½¢çŠ¶",x.shape)
        y = self.linear_relu_stack(x) # torch.Size([64, 1])
        #print("è¾“å‡ºæ—¶æ•°æ®çš„å½¢çŠ¶",logits.shape)
        return y.squeeze(1) # torch.Size([64])





model = MyNeuralNetwork().to(device)
#print(model)

#loss_fn = nn.CrossEntropyLoss()  # æŸå¤±å‡½æ•° nnè°ƒç”¨ç”µç­’ ï¼Œ äº¤å‰ç†µæŸå¤±å‡½æ•° # ç”¨äºåˆ†ç±»é—®é¢˜çš„ï¼Œç¦»æ•£çš„

loss_fn = nn.MSELoss() #ç”¨äºå›å½’ï¼Œè¿ç»­å€¼é—®é¢˜

# optimizer = torch.optim.SGD(model.parameters(), lr=1e-3) # ä¼˜åŒ–å™¨ï¼Œç”µç­’çš„é»˜è®¤è¯­æ³•ï¼šåˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨ SGDä¼˜åŒ–å™¨ # learn rate
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)

def train(dataloader, model, loss_fn, optimizer):
    size = len(dataloader.dataset)
    model.train()
    for batch, (X, y) in enumerate(dataloader):
        X, y = X.to(device), y.to(device)

        # Compute prediction error
        pred = model(X)
        loss = loss_fn(pred, y)

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        if batch % 100 == 0:
            loss, current = loss.item(), (batch + 1) * len(X)
            print(f"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]")


def test(dataloader, model, loss_fn):
    size = len(dataloader.dataset)
    num_batches = len(dataloader)
    model.eval()
    test_loss = 0
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            test_loss += loss_fn(pred, y).item()

    test_loss /= num_batches
    print(f"Test set: Average loss (MSE): {test_loss:>8f}")

    # å¯é€‰ï¼šè®¡ç®—å¹¶æ˜¾ç¤ºå…¶ä»–å›å½’æŒ‡æ ‡ï¼Œå¦‚MAE
    model.eval()
    all_preds = []
    all_targets = []
    with torch.no_grad():
        for X, y in dataloader:
            X, y = X.to(device), y.to(device)
            pred = model(X)
            all_preds.append(pred)
            all_targets.append(y)

    all_preds = torch.cat(all_preds)
    all_targets = torch.cat(all_targets)
    mae = torch.mean(torch.abs(all_preds - all_targets)).item()
    print(f"Test set: MAE: {mae:>8f}")

# éœ€è¦è®­ç»ƒçš„æ˜¯æƒé‡çŸ©é˜µ

epochs = 100
for t in range(epochs):
    print(f"Epoch {t+1}\n-------------------------------")
    train(train_loader, model, loss_fn, optimizer)
    test(test_loader, model, loss_fn)

print("Done!")

torch.save(model.state_dict(), "model.pth")
print("Saved PyTorch Model State to model.pth")
